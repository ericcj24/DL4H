{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Electrocardiogram (ECG) is a quick test to detect heart beat, it records electric signals of the heart. The result of the measurement help detect irregular heart beat.[9]\n",
        "\n",
        "*   ECG data is produced by various real time monitoring device types manufactured with a wide range of accuracies and noise levels, the primary issues for ECG Classification through traditional DNN methods are -\n",
        "  * The rare occurrence of anomalies in continuous ECG data leads to Class imbalance\n",
        "  * Low signal quality arising due to low sampling frequency.\n",
        "  * Human experts are required to annotate the sample to achieve acceptable accuracy, this expert interaction is an expensive activity.\n",
        "\n",
        "The third limitation is the one this paper targets to solve by applying the specific approach described below. Alternative approaches exist to handle this limitation like Active Learning and semi-supervised learning but the primary method we will be working on is Transfer learning.\n",
        "  \n",
        "  ## What did the paper propose? What is the innovations of the method\n",
        "\n",
        "\n",
        "This paper use Transfer Learning techniques to pretrain the large amount of publicly available raw ECG data in Icentia11K [2] (labeled and unlabeled samples are used as part of pre training tasks) and transfer the weights from the model as an input to CNN which will then use this input to finetune the model and will try to classify Atrial Fibrillation (AF, this is a severe type of heart arrhythmia) on PhysioNet/CinC 2017 input data set [3][4].\n",
        "\n",
        "The concept of Transfer Learning is applied extensively in computer vision for classification algorithms; this relies on the theory that knowledge extracted from a large dataset can be applied in a similar domain to improve the model performance for a completely different kind of classification. This paper pretrains the data for heart failure classification, using the weights as starting point, instead of random weights initialization, to obtain a more accurate AF classification on smaller target dataset in PhysioNet/CinC 2017 data set [3][4].\n",
        "\n",
        "  # How well the proposed method work (in its own metrics)?\n",
        "  The paper indicates an improvement of 6.5% in related ECG classification when model is pretrained on a large dataset.\n",
        "  \n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "List hypotheses from the paper we will test and the corresponding experiments we will run.\n",
        "\n",
        "We have constructed following hypothesis:\n",
        "\n",
        "1.   Hypothesis 1: Beat, Rhythm and Heart Rate classification pre-training methods will have better macro F1 score than the random weights initialization methods when dealing with ECG classification problem.\n",
        "2.   Hypothesis 2: Pre-training will improve the training times in addition to increase in performance when compared to random weights initialization.\n",
        "3.   Hypothesis 3: Pretraining allows models to be trained on less data and still achieve the same degree of performance as the same models that are not pretrained.\n",
        "4.   Hypothesis 4: Future predictions will work with acceptable accuracy when using pre-trained models on downstream datasets like PTB-XL database [5] for a related ECG classification.\n",
        "\n",
        "We will device following experiments to test above hypthesis:\n",
        "\n",
        "1. In experiment 1, We will pre-train the model on Icentia dataset, then transfer the weights for downstream fine-tuning model (model 1), we will separately train a model with weight that is randomly initialiazed (model 2), we will verify our hypothesis 1 that model 1 should have better f1 score.\n",
        "\n",
        "2. During experiment 1, we will record the epoch used in training both models, and test hypothesis 2 by observing which model performs better with same training epoch.\n",
        "\n",
        "3. During experiment 1, we will record the volume of data used in training both models, and test hypothesis 3, by observing the performance difference of the models when data volume used in testing are the same.\n",
        "\n",
        "4. We will proceed to use PTB-XL database to fine-tune the model, and record downstream model (model 3) performance on hold-out dataset.\n",
        "\n",
        "# Out of Scope\n",
        "1. The paper also illustrates the future prediction based on attention model based transformer archietecture, due to sheer scale of work involved, we anticipate we will not be able to test this pretraining hypothesis as part of this paper. We nonetheless try to provide a brief analysis on Model architecture and report on our understanding of the underlying principle of Contrastive Predictive Coding[6].\n",
        "2. One of the paper experiments deals with experimenting with training on ResNet50 and other model parameters like filter size, we will not be able to record the observations for this part of the experiment due to computation and time constraints.\n",
        "\n",
        "![sample_image.png](https://drive.google.com/uc?export=view&id=1g2efvsRJDxTxKz-OY3loMhihrEUdBxbc)\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code comment is used as inline annotations for your coding"
      ],
      "metadata": {
        "id": "ABD4VhFZbehA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.chdir('/content/drive/My Drive/DL4H')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8qy5Za6zhBZN",
        "outputId": "7c760c71-4ff7-47f4-fe87-79c1b8277500"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4iuzC8gmhYi2",
        "outputId": "9b3f9ec4-d73e-46c3-8b07-103511a91bb8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/DL4H\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/DL4H/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mC9b9XljE_mZ",
        "outputId": "e3ec7908-b218-4a31-ff86-009cda0dfbc0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "af-classification-from-a-short-single-lead-ecg-recording-the-physionet-computing-in-cardiology-challenge-2017-1.0.0\n",
            "finetuning\n",
            "jobs\n",
            "pretraining\n",
            "transplant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "You can also use code to display images, see the code below.\n",
        "\n",
        "The images must be saved in Google Drive first.\n"
      ],
      "metadata": {
        "id": "LM4WUjz64C3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can upload it to your google drive and show it with OpenCV or matplotlib\n",
        "'''\n",
        "# mount this notebook to your google drive\n",
        "#drive.mount('/content/gdrive')\n",
        "\n",
        "# define dirs to workspace and data\n",
        "img_dir = '/content/drive/MyDrive/DL4H/transferlearning1.png'\n",
        "\n",
        "img = cv2.imread(img_dir)\n",
        "\n",
        "cv2_imshow(img)\n",
        "\n"
      ],
      "metadata": {
        "id": "rRksCB1vbYwJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "67cda355-6883-467a-e1e3-b4022b011671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'clip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-11ac3364fb25>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/patches/__init__.py\u001b[0m in \u001b[0;36mcv2_imshow\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0man\u001b[0m \u001b[0mNxM\u001b[0m \u001b[0mBGRA\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \"\"\"\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;31m# cv2 stores colors as BGR; convert to RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'clip'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "The core parts of this paper involve the below steps.\n",
        "\n",
        "\n",
        "\n",
        "1.   Step 1 - Pretraining residual CNN on large raw ECG data availabe in Icentia11k dataset[2], the dataset has size roughly 272GB.\n",
        "2.   Step 2 - Finetuning the residual network for ECG classification for a smaller dataset PhysioNet/CinC 2017 data set [3][4] of size 1.4 GB.\n",
        "3.   Step 3 - Use the pretrained model to implement ECG classification on a downstream dataset PTB-XL database [5] for related ECG classification in this case, heart beat classification.\n",
        "4.   Step 4 - If time permits, Study the transformer architecture with attention model to check the future prediction of ECG frames to better use the economical unsupervisied mode for ECG classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Important note - we plan to use the code provided by authors here- https://github.com/kweimann/ecg-transfer-learning/tree/master\n",
        "# we will import the necessary code to display the model, data set statistics and experiments specified in the paper."
      ],
      "metadata": {
        "id": "JAgVYD_Ah5by"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import  packages you need\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below we install some package required for the orignal paper git repo code (repo code).\n",
        "# One thing we noticed is the repo code has a dependency on samplerate, which requires cmake to build.\n",
        "# For some stange reason pip would complain cmake not existing despite the opposite.\n",
        "# We found a solution by unstall cmake first, then install samplerate. This resolves the issue.\n",
        "# Separately, we also need to install another dependency wfdb, which runs smoothly."
      ],
      "metadata": {
        "id": "Xluj98zAhQ51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1\n",
        "!pip uninstall --verbose cmake\n",
        "# step 2\n",
        "!pip install --verbose samplerate\n",
        "# step 3\n",
        "!pip install --verbose wfdb\n",
        "\n",
        "# not used: update torch summary, maybe to print model dimension?\n",
        "!pip install --verbose torchsummary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Akhj3EmgXC2L",
        "outputId": "2e1068ce-aaef-45fd-bc33-8e77147cf84a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping cmake as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mUsing pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Requirement already satisfied: samplerate in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from samplerate) (1.25.2)\n",
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.10/dist-packages (4.1.2)\n",
            "Requirement already satisfied: SoundFile>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (0.12.1)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from wfdb) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from wfdb) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.11.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->wfdb) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->wfdb) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (2024.2.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from SoundFile>=0.10.0->wfdb) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->SoundFile>=0.10.0->wfdb) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.16.0)\n",
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Step 1 - PreTraining\n",
        "\n",
        "Goal - DNN's will generalize better to unseen data after encountering wide variety of ECG signals during pretraining.\n",
        "\n",
        "## Source of Input Data:\n",
        "Publicly available Icentia11K [2], the dataset has size roughly 272GB.\n",
        "Raw Dataset available at : https://academictorrents.com/details/af04abfe9a3c96b30e5dd029eb185e19a7055272\n",
        "\n",
        "\n",
        "## Input Data Statistics[2]\n",
        "\n",
        "\n",
        "* No of Patients:       11000\n",
        "* No of Labeled Beats:  2,774,054,987\n",
        "* Sample Rate:          250 Hz\n",
        "* Frame Size:           2049 Samples\n",
        "* Segment Size:         1,048,577 Samples\n",
        "* No of Frames:         1,084,314\n",
        "* No of Segments:       542,157\n",
        "* Dataset Size:         271.27 GB\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "TODO --\n",
        "\n",
        "3. Downstream Datasets for additional classification tasks - The paper uses two distinct datasets for this part: PTB-XL database [5], and a dataset released in 1st China Physiological Signal Challenge 2018 [8]. In this production attempt, in the interest of brevity, we will explore with PTB-XL dataset, a dataset of size 1.8G in a zip file.\n",
        "\n",
        "\n",
        "## Upload of raw data for pretraining?\n",
        "\n",
        "No, due to large size of input data, we have run the pretraining steps on on our local machine and uploaded the weights to google drive, we plan to use these pre-trained files for rest of our pipelines.\n",
        "\n"
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction and data processing for pretraining\n",
        "#.  1. Select random no of patients.\n",
        "#   2. Collect mini batches by sampling short ECG frames(around 60 seconds)\n",
        "#.  3. Standardize each frame using meand and standard deviation of entire dataset, results in 4096 frames/ Patient.\n",
        "#   4. Pretraining using both resNet18 and resNet34 architecture,\n",
        "       #the paper only uses single epoch but we have pretrained on 10 epochs.\n",
        "#.  5. Pick the best accuracy epoch for extracting weights,\n",
        "       #we will illustratate this from google drive jobs/<beats_mulitple_epochs-<34>_classification/history.csv>"
      ],
      "metadata": {
        "id": "CNanI0u3ue5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Beat Classification - Feature Extraction and data processing for pretraining\n",
        "# from google drive jobs/<beats_mulitple_epochs-<34>_classification/history.csv>\n",
        "# save the checkpoint weights for beat classification for resNet18 and resNet34\n",
        "\n",
        "# Input Data: Short ECG Frame\n",
        "\n",
        "\n",
        "# Classification class Labels : Normal,\n",
        "                    #         : Premature Atrial Contraction,\n",
        "                    #         : Premature Ventrical Contraction\n",
        "\n",
        "from pretraining.utils import get_pretrained_weights\n",
        "resnet18 = get_pretrained_weights(\n",
        "   checkpoint_file='<jobs/beat_classification/epoch_01/model.weights>',\n",
        "   task='beat',\n",
        "   arch='resnet18')\n",
        "resnet18.save_weights('jobs/beat_classification/resnet18.weights')\n",
        "\n",
        "\n",
        "\n",
        "resnet34 = get_pretrained_weights(\n",
        "   checkpoint_file='<jobs/beat_classification/epoch_01/model.weights>',\n",
        "   task='beat',\n",
        "   arch='resnet34')\n",
        "resnet18.save_weights('jobs/beat_classification/resnet34.weights')\n"
      ],
      "metadata": {
        "id": "ta3HkHfpwcQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rhythm Classification - Feature Extraction and data processing for pretraining\n",
        "# from google drive jobs/<rhythm_mulitple_epochs-<34>_classification/history.csv>\n",
        "# save the checkpoint weights for beat classification for resNet18 and resNet34\n",
        "\n",
        "# Input Data: Short ECG Frame\n",
        "\n",
        "# How is this measured? Duration of every rhythm to determine the label.\n",
        "\n",
        "\n",
        "# Classification class Labels : Normal,\n",
        "                    #         : Noise,\n",
        "                    #         : Atrial Fibrillation AFlib\n",
        "                    #         : Atrial Flutter AFlutter\n",
        "\n",
        "resnet18 = get_pretrained_weights(\n",
        "   checkpoint_file='<jobs/rhythm_classification/epoch_01/model.weights>',\n",
        "   task='rhythm',\n",
        "   arch='resnet18')\n",
        "resnet18.save_weights('jobs/rhythm_classification/resnet18.weights')\n",
        "\n",
        "\n",
        "\n",
        "resnet34 = get_pretrained_weights(\n",
        "   checkpoint_file='<jobs/rhythm_classification/epoch_01/model.weights>',\n",
        "   task='rhythm',\n",
        "   arch='resnet34')\n",
        "resnet18.save_weights('jobs/rhythm_classification/resnet34.weights')\n"
      ],
      "metadata": {
        "id": "UXUJrQsdwscx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heart Rate Classification - Feature Extraction and data processing for pretraining\n",
        "# from google drive jobs/<hr_mulitple_epochs-<34>_classification/history.csv>\n",
        "# save the checkpoint weights for beat classification for resNet18 and resNet34\n",
        "\n",
        "# What is measured? Estimate no of Beats/ Min\n",
        "\n",
        "# How is this feature extracted?\n",
        "#                     The labels are auto generated.\n",
        "#                     find indices of heart beat using QRS Algorithm[35 in paper]\n",
        "#                     Extend frame by 1 second at both ends.\n",
        "\n",
        "# Input Data: Short ECG Frame\n",
        "\n",
        "# Classification class Labels : Normal (60-100),\n",
        "                    #         : Noise,\n",
        "                    #         : Tachycardia (>100)\n",
        "                    #         : Bradycardia (<60)\n",
        "\n",
        "resnet18 = get_pretrained_weights(\n",
        "   checkpoint_file='<jobs/hr_classification/epoch_01/model.weights>',\n",
        "   task='hr',\n",
        "   arch='resnet18')\n",
        "resnet18.save_weights('jobs/hr_classification/resnet18.weights')\n",
        "\n",
        "\n",
        "\n",
        "resnet34 = get_pretrained_weights(\n",
        "   checkpoint_file='<jobs/hr_classification/epoch_01/model.weights>',\n",
        "   task='hr',\n",
        "   arch='resnet34')\n",
        "resnet18.save_weights('jobs/hr_classification/resnet34.weights')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "4ZNaA2zPw1g2",
        "outputId": "2eaf2921-c30d-4c8f-f2e0-c4057e042f6f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'get_pretrained_weights' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8660143e24f9>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m resnet18 = get_pretrained_weights(\n\u001b[0m\u001b[1;32m      7\u001b[0m    \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'<jobs/hr_classification/epoch_01/model.weights>'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m    \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_pretrained_weights' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "# dir and function to load raw data\n",
        "#raw_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
        "\n",
        "# def load_raw_data(raw_data_dir):\n",
        "#   # implement this function to load raw data to dataframe/numpy array/tensor\n",
        "#   return None\n",
        "\n",
        "# raw_data = load_raw_data(raw_data_dir)\n",
        "\n",
        "# # calculate statistics\n",
        "# def calculate_stats(raw_data):\n",
        "#   # implement this function to calculate the statistics\n",
        "#   # it is encouraged to print out the results\n",
        "#   return None\n",
        "\n",
        "# # process raw data\n",
        "# def process_data(raw_data):\n",
        "#     # implement this function to process the data as you need\n",
        "#   return None\n",
        "\n",
        "# processed_data = process_data(raw_data)\n",
        "\n",
        "''' you can load the processed data directly\n",
        "processed_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
        "def load_processed_data(raw_data_dir):\n",
        "  pass\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model\n",
        "The paper uses two main models.\n",
        "\n",
        "1.   Residual Network CNN for pretraining and finetuning.\n",
        "2.   Attention based transformer architecture for unsupervised learning.\n",
        "\n",
        "The core claim and contribution of the paper deals with applying transfer learning from large datasets to accomplish ECG classifications on smaller datasets where the need for supervised labels is minimized. For this we only need to study the Residual network for both pretraining and finetuning modes.\n",
        "\n",
        "We will pretrain our model on two architecture types resNet18 and resNet34, the computation and layers of resNet50 would be too complex to train and evaluate due to size of input dataset and compute capacity limitations.\n",
        "\n",
        "\n",
        "\n",
        "TODO\n",
        "  * Model architecture: layer number/size/type, activation function, etc\n",
        "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it.\n",
        "\n",
        "\n",
        "\n",
        "The paper proposes a Transfer Learning structure. It learns on large amount of data, but with less annotation, as a pretraining, then on a smaller, but better curated dataset, we transfer the weights learnt from the pre-training stage, and fine-tuned it for the downstream tasks.\n",
        "\n",
        "For the pretraining phase, the paper uses a ResNet18 structure, trained on Icentia11K dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretraining - Residual Network, ResNet18 considered as shallow network\n",
        "\n",
        "#%run https://github.com/kweimann/ecg-transfer-learning/blob/master/finetuning/utils.py\n",
        "\n",
        "# The core residual network comes from the well documented base network here[6] -\n",
        "# He, K., Zhang, X., Ren, S., & Sun, J. Identity Mappings in Deep Residual Networks (2016). arXiv:1603.05027.\n",
        "\n",
        "\n",
        "# The following modifications are made from the base Model\n",
        "#. 1. Replace 2-d Convolution layers with 1-d counterparts for ECG data.\n",
        "#  2. Larger filter sizes 7,5,5,3 at each stage instead of default size 3.\n",
        "#\n",
        "\n",
        "\n",
        "from transplant.modules.resnet1d import ResNet\n",
        "\n",
        "stages=None\n",
        "resnet_18 = ResNet(num_outputs=None,\n",
        "                        blocks=(2, 2, 2, 2)[:stages],\n",
        "                        kernel_size=(7, 5, 5, 3),\n",
        "                        include_top=False)\n",
        "\n",
        "input_shape = (1,1,64)\n",
        "resnet_18.build(input_shape)\n",
        "print(resnet_18.summary())"
      ],
      "metadata": {
        "id": "Rwgko7f04SQs",
        "outputId": "0ae17bd1-0c80-4853-d0e2-52c7d5a94966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'samplerate'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-905ca78ac603>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# The core residual network comes from the well documented base network here -\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransplant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet1d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/DL4H/transplant/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtransplant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransplant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransplant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransplant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/DL4H/transplant/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransplant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0micentia11k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtransplant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphysionet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/DL4H/transplant/datasets/physionet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwfdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwfdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'samplerate'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# overview of pretraining model (which is a resnet 18 based model)\n",
        "#%run https://raw.githubusercontent.com/kweimann/ecg-transfer-learning/master/transplant/modules/resnet1d.py\n",
        "\n",
        "\n",
        "from transplant.modules.resnet1d import ResNet\n",
        "\n",
        "\n",
        "resnet = ResNet(num_outputs=1,\n",
        "                        blocks=(2, 2, 2, 2)[:None],\n",
        "                        kernel_size=(7, 5, 5, 3),\n",
        "                        include_top=False)\n",
        "input_shape = (1,1,64)\n",
        "resnet.build(input_shape)\n",
        "print(resnet.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "BdL6UNoshX8H",
        "outputId": "5fc9db9e-c182-4eb2-d8aa-99d9722906db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'samplerate'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-15f2ffabe8f4>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransplant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet1d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/DL4H/transplant/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtransplant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransplant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransplant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransplant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/DL4H/transplant/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransplant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0micentia11k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtransplant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphysionet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/DL4H/transplant/datasets/physionet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwfdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwfdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'samplerate'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class my_model():\n",
        "#   # use this class to define your model\n",
        "#   pass\n",
        "\n",
        "# model = my_model()\n",
        "loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer =tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def train_model_one_iter(model, loss_func, optimizer):\n",
        "  pass\n",
        "\n",
        "num_epoch = 10\n",
        "# model training loop: it is better to print the training/validation losses during the training\n",
        "for i in range(num_epoch):\n",
        "  train_model_one_iter(model, loss_func, optimizer)\n",
        "  train_loss, valid_loss = None, None\n",
        "  print(\"Train Loss: %.2f, Validation Loss: %.2f\" % (train_loss, valid_loss))\n"
      ],
      "metadata": {
        "id": "gBdVZoTvsSFV",
        "outputId": "254a7a76-a425-43e9-9c8b-68eb5c26fa1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "must be real number, not NoneType",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-ffd7a672b142>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mtrain_model_one_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Loss: %.2f, Validation Loss: %.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: must be real number, not NoneType"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 - FineTuning Data and Model Section\n",
        "\n",
        "## Input data:\n",
        "\n",
        "Smaller target dataset for AF classification - PhysioNet/CinC 2017 data set [3][4].\n",
        "Sample Rate:            300 Hz\n",
        "Dataset Size:           1.4 GB\n",
        "Test dataset available: No\n",
        "Duration of Signal:     9-60 seconds.\n",
        "\n",
        "\n",
        "## Data Split:\n",
        "For the purpose of fine-tuning, the dataset has been splitted to 80/20 between training and testing.\n",
        "\n",
        "## Target ECG classification\n",
        "AF classification\n",
        "\n",
        "## Location of dataset\n",
        "\n",
        "Uploaded the file to Gdrive and attached to this notebook as a mount."
      ],
      "metadata": {
        "id": "uVmwZ4nM8cgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2 Finetuning\n",
        "\n",
        "#Data preprocessing Steps:\n",
        "\n",
        "# 1. Standardize signal using mean and standard deviation across the dataset.\n",
        "# 2. Down Sampling from 300 to 250 Hz to match the Icentia11k dataset.\n",
        "# 3. Pad some recording with zeros to have signal length of 60 seconds.\n",
        "# 4. 75% training data, 20% Test and 5% validation splits with class balanced ratios.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "38CNudGj-HnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2 - FineTuning Model Modifications\n",
        "\n",
        "#Pre - FineTuning Steps:\n",
        "\n",
        "\n",
        "# Replace CNN output layer with Fully connected later with random weights,\n",
        "# ensure the output matches the classes of finetuning dataset.\n",
        "\n",
        "\n",
        "# FineTuning - Start\n",
        "\n",
        "#Train CNN for upto 200 epochs, 50 epochs limit for accuracy metric improvement.\n",
        "# If no imporvement for 50 epochs, stop training.\n",
        "\n",
        "\n",
        "# Record macro F1 Score.\n",
        "\n",
        "\n",
        "# FineTuning - End.\n",
        "\n",
        "# Revert weights of network to checkpoint at model at highest macro F1 score\n",
        "# on Val set.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Finally\n",
        "\n",
        "# Record macro F1 score on Test Set.\n",
        "\n",
        "#end Finetuning"
      ],
      "metadata": {
        "id": "nkRtTJq5-71f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Pre Training methods\n",
        "\n",
        "We have following pre training methods for evaluation\n",
        "\n",
        "\n",
        "\n",
        "1.   Beat Classification\n",
        "2.   Hr Classification\n",
        "3.   Rhythm Classification.\n",
        "4.   Stretch Goal - Future Prediction based on Transformer Architecture.\n",
        "\n",
        "## Abalations Scope Evaluation\n",
        "\n",
        "\n",
        "\n",
        "1.   Remove PreTraining and try ECG classification on af classification dataset, so random weights method.\n",
        "\n"
      ],
      "metadata": {
        "id": "aEnnpO7tAv17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of pre training methods\n",
        "\n",
        "# For each method call finetuning in above section.\n",
        "\n",
        "#. Epoch : 10 times.\n",
        "#.        For each Run:\n",
        "#                      1. new Train and validation sets.\n",
        "#                      2. adjust output layers based on method. beat vs hr vs rhythm.\n",
        "#                      3. finetune model.\n",
        "#                      4. record macro f1 score on Test Set.\n",
        "#"
      ],
      "metadata": {
        "id": "4bQ7Jg_SBe80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
        "\n",
        "Please test and report results for all experiments that you run with:\n",
        "\n",
        "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
        "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics to evaluate my model\n",
        "\n",
        "# plot figures to better show the results\n",
        "\n",
        "# it is better to save the numbers and figures for your presentation."
      ],
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare you model with others\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ],
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "  * What will you do in next phase.\n",
        "\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can read and plot it here like the Scope of Reproducibility\n",
        "'''"
      ],
      "metadata": {
        "id": "E2VDXo5F4Frm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Notebook to Google Drive\n",
        "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
        "\n",
        "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"
      ],
      "metadata": {
        "id": "dlv6knX04FiY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1. Weimann, K., Conrad, T.O.F. Transfer learning for ECG classification. Sci Rep 11, 5251 (2021). https://doi.org/10.1038/s41598-021-84374-8 https://github.com/kweimann/ecg-transfer-learning/tree/master\n",
        "\n",
        "2. Tan, S. et al. Icentia11K: An Unsupervised Representation Learning Dataset for Arrhythmia Subtype Discovery (2019). arXiv:arXiv:1910.09570\n",
        "\n",
        "3. Clifford, G. D. et al. AF Classification from a short single lead ECG recording: the PhysioNet/computing in cardiology challenge 2017. Comput. Cardiol. (2017).In 2017 Computing in Cardiology (CinC) 2017 Sep 24 (pp. 1-4). IEEE. https://doi.org/10.22489/CinC.2017.065-469\n",
        "\n",
        "4. Goldberger, A. L. et al. PhysioBank, PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals. Circulation 101, e215–e220. https://doi.org/10.1161/01.CIR.101.23.e215 (2000).\n",
        "\n",
        "5. Strodthoff, N., Wagner, P., Schaeffter, T., & Samek, W. Deep Learning for ECG Analysis: Benchmarks and Insights from PTB-XL (2020). arXiv:2004.13701.\n",
        "\n",
        "6. van den Oord, A., Li, Y., & Vinyals, O. Representation Learning with Contrastive Predictive Coding (2019). arXiv:1807.03748.\n",
        "\n",
        "7. He, K., Zhang, X., Ren, S., & Sun, J. Deep Residual Learning for Image Recognition (2015). arXiv:1512.03385.\n",
        "\n",
        "8. Liu, F. et al. An open access database for evaluating the algorithms of electrocardiogram rhythm and morphology abnormality detection. J. Med. Imaging Health Inf. 8, 1368–1373. https://doi.org/10.1166/jmihi.2018.2442 (2018).\n",
        "\n",
        "9. https://www.mayoclinic.org/tests-procedures/ekg/about/pac-20384983\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feel free to add new sections"
      ],
      "metadata": {
        "id": "xmVuzQ724HbO"
      }
    }
  ]
}